{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">✗</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">✗</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get ready for this lesson by importing the packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "import ultralytics\n",
    "from IPython.display import Video\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll print out the version numbers for our libraries, including Python. We want to make sure that anyone who reviews our work knows exactly what software we used in case they want to reproduce our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"PIL version : \", PIL.__version__)\n",
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)\n",
    "print(\"ultralytics version : \", ultralytics.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image detection with YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous project, we solved the problem of detecting if an image contains one object from a set of objects. For example, does the image contain a hog or not. But we know that images rarely just contain one object. Images often contain different objects and multiples of the same object. Imagine an image of a traffic scene that contains cars, trucks, pedestrians, traffic signs, and more. We need to use an object detection model. Object detection is the process of identifying and locating objects in an image or video.\n",
    "\n",
    "We won't train an object detection algorithm from scratch but instead use a pretrained model. We'll use the YOLO (You Only Look Once) model. It's popular for its speed and accuracy. Lucky for us, the `ultralytics` package contains the YOLO model. We'll use the YOLO version 8 pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO(task=\"detect\", model=\"yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What classes can this pretrained model detect? That's stored in `yolo.names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a dictionary that maps an integer to the class label. Let's see what 5 corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.names[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.1:** Determine the class that's assigned to integer 23?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_assigned_to_23 = yolo.names[23]\n",
    "print(f\"{class_assigned_to_23} corresponds to 23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task involves identifying objects from traffic video feeds. There are several objects we want to detect that are not included in the classes from the pretrained YOLO model. These classes are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_not_in_yolo = [\n",
    "    \"ambulance\",\n",
    "    \"army vehicle\",\n",
    "    \"auto rickshaw\",\n",
    "    \"garbagevan\",\n",
    "    \"human hauler\",\n",
    "    \"minibus\",\n",
    "    \"minivan\",\n",
    "    \"pickup\",\n",
    "    \"policecar\",\n",
    "    \"rickshaw\",\n",
    "    \"scooter\",\n",
    "    \"suv\",\n",
    "    \"taxi\",\n",
    "    \"three wheelers (CNG)\",\n",
    "    \"van\",\n",
    "    \"wheelbarrow\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that \"ambulance\" is not in the YOLO classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ambulance\" not in yolo.names.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.2:** Double check that \"army vehicle\" is not in the YOLO classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_army_vehicle_inlcuded = \"army vehicle\" not in yolo.names.values()\n",
    "print(is_army_vehicle_inlcuded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a later lesson, we'll retrain the YOLO model to include the missing classes. For this lesson, we are OK with what's already provided. We are most interested in the first 13 classes. Those classes are objects often found in traffic. \n",
    "\n",
    "Let's use the YOLO model to identify objects in one frame of our video data. We'll use `Path` provided by `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data_video\", \"extracted_frames\")\n",
    "image_path = data_dir / \"frame_1050.jpg\"\n",
    "\n",
    "result = yolo(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `result`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type of result: {type(result)}\")\n",
    "print(f\"Length of result: {len(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's a list of length 1. We'll explore its contents shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.3:** Run the YOLO model on `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_task = data_dir / \"frame_2575.jpg\"\n",
    "result_task = yolo(image_path_task)\n",
    "\n",
    "print(type(result_task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another way to use the YOLO model. It's to use the object's `.predict` method. The advantage is that it's clearer what we're doing and allows us to overwrite any default values when predicting. For example, we can control the confidence value of the resulting bounding boxes. Let's use the `.predict` method and specify a 50% threshold for the bounding box and save the results to disk as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = yolo.predict(image_path, conf=0.5, save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are contained in the created `runs` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.4:** Use the `predict` method for `frame_2575.jpg`. Make sure you use a 50% confidence threshold and save the results as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task = yolo.predict(image_path_task, conf=0.5, save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results From Running YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`result[0]` contains a special object with the results of the prediction stored as attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll break these results down further.\n",
    "\n",
    "`.boxes` contains the data for the bounding boxes. These bounding boxes are the main things we want from object detection. These boxes are then used to create a box around the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to further unpack what's inside the `.boxes` attribute. The `.cls` attribute contains the classes of each of the objects detected. It's a PyTorch tensor. The length of the tensor is the number of objects detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].boxes.cls)\n",
    "print(f\"Number of objects detected: {len(result[0].boxes.cls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these numbers are mapped to the name of the classes. For example, 0 corresponds to \"person\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.5:** Determine the number of detected objects in `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_detected_objs = len(result_task[0].boxes.cls)\n",
    "print(f\"Number of objects detected in frame_2575.jpg: {number_of_detected_objs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the objects we detected. The keys of `yolo.names` are integers so we'll need to cast the floats in `result[0].boxes.cls` to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_counts = Counter([yolo.names[int(cls)] for cls in result[0].boxes.cls])\n",
    "object_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"car\" class was the most common, followed by \"person\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.6:** Determine the most common class and the number of times it was detected in `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_counts_task = Counter([yolo.names[int(cls)] for cls in result_task[0].boxes.cls])\n",
    "\n",
    "most_common_class, count_of_class = object_counts_task.most_common(1)[0]\n",
    "print(f\"Most common class: {most_common_class}\")\n",
    "print(f\"Number of detected {most_common_class}: {count_of_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important attribute is `.conf` which has the confidence of the detected bounding boxes. The confidence is stored in a PyTorch tensor. We should expect this tensor's length to match the number we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].boxes.conf)\n",
    "print(f\"Number of objects detected: {len(result[0].boxes.conf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.7:** Check the length of the confidence tensor of `result_task` to verify this number matches to what was observed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_confidence_tensor = len(result_task[0].boxes.conf)\n",
    "print(f\"Number of objects detected: {length_of_confidence_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling `.predict`, we set the confidence threshold to 50%. That is why all values in the confidence tensor is greater than 0.5. How many of the bounding boxes have a confidence value greater than 75%? For frame `frame_1050.jpg`, that would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_confident_objects = (result[0].boxes.conf > 0.75).sum().item()\n",
    "print(f\"Number of objects detected with 50% confidence: {number_of_confident_objects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.8:** Calculate the number of objects that were detected in `frame_2575.jpg` with 75% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_confident_objects_task = (result_task[0].boxes.conf > 0.75).sum().item()\n",
    "\n",
    "print(\n",
    "    f\"Number of objects detected in frame_2575.jpg with 50% confidence: {number_of_confident_objects_task}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.data` attribute contains the raw detection data. We won't be using it as there are attributes with the bounding box data in an easier to use form. `.orig_shape` is just the original shape of the input. The attribute `is_track` indicates whether object tracking has been turned on. This is useful when we want to track an object across multiple frames. What follows next are the attributes that store the processed bounding boxes. They are provided in four different forms. All these forms describe the box using four values. The different forms will help us if we are using a tool where the bound box can only be one particular format.\n",
    "\n",
    "We'll go through all four of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.xywh` is a tensor with four columns for each row. Each row represents one box. The first and second column is the x and y coordinates of the top-left corner of the box, respectively. The third and fourth columns are width and height, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result[0].boxes.xywh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.xywhn` is very similar to `.xywh` but these coordinates have been normalized by the image size. We can remind ourselves of the original shape with `.orig_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].orig_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the image is 360 pixels high and 640 pixels wide. Let's examine one row of the normalized bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].boxes.xywhn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the original shape to verify that indeed `.xywhn` is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].boxes.xywh[0] / torch.Tensor([640, 360, 640, 360]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That matches from what we saw earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.9:** Print out the original shape of `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape_task = result_task[0].orig_shape\n",
    "print(f\"Original shape of frame_2574.jpg: {original_shape_task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.10:** Print out the normalized `xywh` bounding box for the first object of `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_xywh = result_task[0].boxes.xywhn\n",
    "print(f\"Normalized xywh bounding box for frame_2575.jpg: {normalized_xywh[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.11:** Normalize the bounding box using the original shape of the `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_xywh_task = result_task[0].boxes.xywhn / torch.Tensor([640, 360, 640, 360]).to(\"cuda\")\n",
    "print(f\"Normalized xywh bounding box for frame_2575.jpg: {normalized_xywh[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third provided bounding box form is `.xyxy`. This form contains two coordinates, the (x, y) coordinate for the top left corner and the (x, y) coordinate of the bottom right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].boxes.xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last form is `.xyxyn` which is the normalized form of `.xyxy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].boxes.xyxyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored the most important attributes of the `.boxes` attribute of the returned result object. Now let's return the remaining important attributes. `.save_dir` is just the location where we've saved the resulting bounding boxes. We'll use the method `exists` of a `Path` object to make sure the location actually exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_results = Path(result[0].save_dir)\n",
    "\n",
    "print(f\"Results saved to {location_of_results}\")\n",
    "location_of_results.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.12:** Determine the location for the results of `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_results_task = Path(result_task[0].save_dir)\n",
    "print(f\"Results for frame_2575.jpg saved to {location_of_results_task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `.speed` is a dictionary for the time it took to run the preprocessing, inference (prediction), and postprocessing steps. These times are measured in milliseconds. A good rule of thumb is that times less than 100 milliseconds are experienced as instantaneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total time in milliseconds: {sum(result[0].speed.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.13:** Calculate the total time object detection took for `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = sum(result_task[0].speed.values())\n",
    "print(f\"Total time in milliseconds: {total_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving our results, we've created an image file with the bounding boxes drawn in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(location_of_results / \"frame_1050.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each class uses a different color, the labels are displayed, along with the confidence of the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.14:** Display image `frame_2575.jpg` with its drawn bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image frame_2575.jpg with the bounding boxes\n",
    "Image.open(location_of_results_task / \"frame_2575.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding boxes were saved as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (location_of_results / \"labels\" / \"frame_1050.txt\").open(\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the class, followed by 4 columns defining the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.15:** Display the text file results for the bounding box for `frame_2575.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (location_of_results_task / \"labels\" / \"frame_2575.txt\").open(\"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using YOLO on Multiple Images and Video Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to move on to using YOLO for identifying objects across multiple images. For convenience, we'll define a function that accepts a directory of images and displays them in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(dir_path, sample=5):\n",
    "    dir_path = Path(dir_path) if isinstance(dir_path, str) else dir_path\n",
    "\n",
    "    image_list = []\n",
    "    # Sort the images to ensure they are processed in order\n",
    "    images = sorted(dir_path.glob(\"*.jpg\"))\n",
    "    if not images:\n",
    "        return None\n",
    "\n",
    "    # Iterate over the first 'sample' images\n",
    "    for img_path in images[:sample]:\n",
    "        img = read_image(str(img_path))\n",
    "        resize_transform = transforms.Resize((240, 240))\n",
    "        img = resize_transform(img)\n",
    "        image_list.append(img)\n",
    "\n",
    "    # Organize the grid to have 'sample' images per row\n",
    "    Grid = make_grid(image_list, nrow=5)\n",
    "    # Convert the tensor grid to a PIL Image for display\n",
    "    img = torchvision.transforms.ToPILImage()(Grid)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function defined, let's use it for the first 25 frames we extracted from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_images(data_dir, sample=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.16:** Use `display_sample_images` to display the first ten frames in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first ten images\n",
    "display_sample_images(data_dir, sample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a list of the path of 25 images from the extracted frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = list(data_dir.iterdir())[:25]\n",
    "images_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.17:** Create a list of the _last_ ten frames as listed by `data_dir.iterdir()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path_task = list(data_dir.iterdir())[:10]\n",
    "\n",
    "print(f\"Number of frames in list: {len(images_path_task)}\")\n",
    "images_path_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again use `yolo.predict` but this time we'll make use of two additional arguments to control where the results are saved. By using `project` and `name`, the saved results will be in `project/name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = yolo.predict(\n",
    "    images_path,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    save_txt=True,\n",
    "    project=Path(\"runs\", \"detect\"),\n",
    "    name=\"multiple_frames\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0].save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the output includes a summary of the results for each of the 25 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.18:** Use `yolo.predict` on `images_path_task`. Save the results to `runs/detect/multiple_frames_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_task = yolo.predict(\n",
    "    images_path_task,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    save_txt=True,\n",
    "    project=Path(\"runs\", \"detect\"),\n",
    "    name=\"multiple_frames\",\n",
    ")\n",
    "\n",
    "print(f\"\\nResults from task saved to: {results_task[0].save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `display_sample_images` function, we can display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = display_sample_images(results[0].save_dir, sample=25)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how YOLO did a good job at detecting the different objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.19:** Display the images with the bounding boxes with `display_sample_images` for the results generated in the previous task. Make sure to set `sample` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_task = display_sample_images(results_task[0].save_dir, sample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use YOLO on a video source instead of the frames extracted from a video. The cell below displays the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = Path(\"data_video\", \"dhaka_traffic.mp4\")\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we're going to truncate our video and run YOLO against the truncated version. We'll use `ffmpeg`, a command line tool for video and audio editing. The part that controls the timestamps for truncation are the numbers that follow `-ss` and `-to`. The number after `-ss` is the starting timestamp and `-to` is the ending timestamp. The value `data_video/dhaka_traffic_truncated.mp4` is the path of the created file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -ss 00:00:00 -to 00:00:30 -y -i $video_path -c copy data_video/dhaka_traffic_truncated.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_truncated_path = Path(\"data_video\", \"dhaka_traffic_truncated.mp4\")\n",
    "Video(video_truncated_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.20:** Truncate the same video as above but from the `00:00:30` to `00:01:00` timestamp and name the video `data_video/dhaka_traffic_truncated_task.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -ss 00:00:30 -to 00:01:00 -y -i $video_path -c copy data_video/dhaka_traffic_truncated_task.mp4\n",
    "\n",
    "video_truncated_path_task = Path(\"data_video\", \"dhaka_traffic_truncated_task.mp4\")\n",
    "Video(video_truncated_path_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use YOLO on a video source, we just need to tell it the location of the video and set `stream` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video = yolo.predict(\n",
    "    video_truncated_path,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    stream=True,\n",
    "    project=Path(\"runs\", \"detect\"),\n",
    "    name=\"video_source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike before, the returned value of `yolo.predict` is a generator rather than a list. Detection happens only as we iterate over the generator, giving us control over when the actual computation takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_video:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saved the results, YOLO created a video. In the next section, we'll look at the video that YOLO produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using YOLO in the Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO has a command line interface. This is great if we are working with shell scripts. You can see how its usage is very similar to what we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict conf=0.5 model=yolov8s.pt source=$video_truncated_path project=\"runs/detect\" name=\"command_line\" > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "If you look at the end, you'll see <code>> /dev/null</code>. This redirects the output of running YOLO from the screen to the device called null. This device is basically a black hole where it destroys anything written to it. The output that would've appeared would've been the same as we saw earlier when running YOLO in Python.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO will create a video of the source video with the bounding boxes. Before we display the video, we'll need to convert it to an mp4 as that format provides better compression. Better compression leads to a smaller file size. The notebook environment might have issues with playing large files. Once again, `ffmpeg` is the tool to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i runs/detect/command_line/dhaka_traffic_truncated.avi output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the video contains the bounding boxes on the detected objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3.21:** Use YOLO in the command line for the truncated video. You'll need to change `source` to be `$video_truncated_path_task` and the `name` to be `command_line_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict conf=0.5 model=yolov8s.pt source=$video_truncated_path_task project=\"runs/detect\" name=\"command_line_task\" > /dev/null\n",
    "\n",
    "# This will convert your video to mp4 and display it in the notebook\n",
    "!ffmpeg -y -i runs/detect/command_line_task/dhaka_traffic_truncated_task.avi output_task.mp4\n",
    "Video(\"output_task.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
